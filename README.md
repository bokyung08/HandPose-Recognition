# HandPose Recognition (1ì¸ì¹­ ì‹¤ì‹œê°„ ì†ë™ì‘ ì¸ì‹)

ì´ í”„ë¡œì íŠ¸ëŠ” **1ì¸ì¹­ ì‹œì  ì†ë™ì‘ ì˜ìƒ**ê³¼ **MediaPipe ì† ëœë“œë§ˆí¬**ë¥¼ í•¨ê»˜ í™œìš©í•˜ì—¬  
**3D CNN (ì˜ìƒ ê¸°ë°˜) + LSTM (ëœë“œë§ˆí¬ ê¸°ë°˜) + Attention ìœµí•© ëª¨ë¸**ì„ í•™ìŠµí•˜ê³ ,  
ì‹¤ì‹œê°„ìœ¼ë¡œ ì†ë™ì‘ì„ ì¸ì‹í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

---

## ğŸ—ï¸ ëª¨ë¸ ì•„í‚¤í…ì²˜ (Model Architecture)

ì´ ëª¨ë¸ì€ ì˜ìƒ í”„ë ˆì„(Video)ê³¼ ì† ì¢Œí‘œ(Landmark) ë°ì´í„°ë¥¼ ê²°í•©í•˜ì—¬ ì¶”ë¡ í•˜ëŠ” ë©€í‹°ëª¨ë‹¬ êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤.

```mermaid
graph LR
    %% ìŠ¤íƒ€ì¼ ì •ì˜ (ë…¼ë¬¸ìš© ê¹”ë”í•œ ìŠ¤íƒ€ì¼)
    classDef input fill:#ffffff,stroke:#000000,stroke-width:2px;
    classDef conv fill:#dae8fc,stroke:#6c8ebf,stroke-width:2px;
    classDef lstm fill:#d5e8d4,stroke:#82b366,stroke-width:2px;
    classDef att fill:#ffe6cc,stroke:#d79b00,stroke-width:2px;
    classDef dense fill:#f8cecc,stroke:#b85450,stroke-width:2px;
    classDef out fill:#e1d5e7,stroke:#9673a6,stroke-width:2px;

    %% 1. ì…ë ¥ ì •ì˜
    subgraph Inputs [Input Data]
        direction TB
        VI(Video Sequence<br/>(T, H, W, 1)):::input
        LI(Landmark Sequence<br/>(T, Joints x 3)):::input
    end

    %% 2. Video Stream (Spatiotemporal)
    subgraph VideoStream [Stream 1: Spatiotemporal Feature Extraction]
        direction LR
        VI --> C1[Conv3D (32)<br/>+ MaxPool]:::conv
        C1 --> C2[Conv3D (64)<br/>+ MaxPool]:::conv
        C2 --> C3[Conv3D (128)<br/>+ MaxPool]:::conv
        C3 --> TDF[TimeDistributed<br/>Flatten]:::conv
        TDF --> VL[LSTM (128)]:::lstm
        VL --> VA[Attention<br/>Mechanism]:::att
        VA --> VC(Video Context):::input
    end

    %% 3. Landmark Stream (Temporal)
    subgraph LandmarkStream [Stream 2: Temporal Feature Extraction]
        direction LR
        LI --> LL[LSTM (128)]:::lstm
        LL --> LA[Attention<br/>Mechanism]:::att
        LA --> LC(Landmark Context):::input
        LC --> LD[Dense (128)<br/>ReLU]:::dense
    end

    %% 4. Fusion & Classification
    subgraph FusionLayer [Multimodal Fusion & Classification]
        direction LR
        VC --> CON[Concatenate]:::dense
        LD --> CON
        CON --> FC1[Dense (128)<br/>ReLU]:::dense
        FC1 --> DO[Dropout (0.5)]:::dense
        DO --> OUT(Softmax<br/>Classifier):::out
    end

    %% ì—°ê²°ì„  ìŠ¤íƒ€ì¼
    linkStyle default stroke:#333,stroke-width:1.5px;
```

Video Branch: 3D CNNì„ í†µí•´ ì˜ìƒì˜ ì‹œê³µê°„ì  íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê³ , LSTMê³¼ Attentionì„ í†µí•´ ì¤‘ìš”í•œ í”„ë ˆì„ ì •ë³´ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.

Landmark Branch: MediaPipeë¡œ ì¶”ì¶œëœ ì† ê´€ì ˆ ì¢Œí‘œì˜ ì‹œê³„ì—´ ë³€í™”ë¥¼ LSTMê³¼ Attentionìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.

Fusion: ë‘ ê°€ì§€ ì •ë³´ë¥¼ ê²°í•©(Concatenate)í•˜ì—¬ ìµœì¢… ì œìŠ¤ì²˜ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤.

ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡°
```

handpose-recognition/
â”‚â”€â”€ README.md               # í”„ë¡œì íŠ¸ ì„¤ëª… ë° ëª¨ë¸ êµ¬ì¡°
â”‚â”€â”€ requirements.txt        # í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª©ë¡
â”‚â”€â”€ config.py               # ê³µí†µ ì„¤ì • (Sequence Length, Image Size ë“±)
â”‚
â”œâ”€â”€ data/                   # ë°ì´í„° ì €ì¥ í´ë”
â”‚   â””â”€â”€ raw/                # ì›ë³¸ ì˜ìƒ ë°ì´í„° (í´ë˜ìŠ¤ë³„ í´ë”)
â”‚
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ data_loader.py      # ë°ì´í„° ë¡œë”© & ì „ì²˜ë¦¬ (Generator)
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ attention_layer.py  # Custom Attention ë ˆì´ì–´ ì •ì˜
â”‚   â”œâ”€â”€ multimodal_model.py # (3D CNN + LSTM + Attention) ìœµí•© ëª¨ë¸
â”‚   â””â”€â”€ multimodal_model_3d.py # (êµ¬ë²„ì „/ëŒ€ì²´ ëª¨ë¸ ë“±)
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ mediapipe_utils.py  # MediaPipe í—¬í¼ í•¨ìˆ˜ (ëœë“œë§ˆí¬ ì¶”ì¶œ)
â”‚   â”œâ”€â”€ visualization.py    # ì‹œê°í™” í•¨ìˆ˜ (í•™ìŠµê³¡ì„ , í˜¼ë™í–‰ë ¬)
â”‚
â”œâ”€â”€ train.py                # ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ evaluate.py             # ëª¨ë¸ í‰ê°€ (ë¦¬í¬íŠ¸ & confusion matrix ì €ì¥)
â””â”€â”€ inference.py            # ì‹¤ì‹œê°„ ì›¹ìº  ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
```
